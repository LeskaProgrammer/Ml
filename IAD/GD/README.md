# HW05 — градиентный спуск

## Что здесь сделано

### Часть 1: игрушечный 2D-датасет

- Введён абстрактный интерфейс `BaseLoss` и реализован MSE-лосс с явным градиентом
- Написан классический градиентный спуск `gradient_descent` и функция визуализации траектории на линиях уровня MSE
- Прогон градиентного спуска с разными `learning rate`:
  - где спуск уверенно сходится
  - где начинает прыгать вокруг минимума
  - где шаг слишком маленький и всё едет очень медленно
- Реализован стохастический градиентный спуск `stochastic_gradient_descent` с мини-батчами и степенным затуханием шага 
  \(lr_t = lr · (s0 / (s0 + t))^p\), плюс сравнение траекторий GD vs SGD

### Часть 2: линейная регрессия на реальных данных

Задача: предсказать цену автомобиля по его характеристикам на датасете UCI Automobile

Подготовка данных:

- удалены объекты без целевой переменной (цены)
- числовые пропуски заполнены средними значениями по столбцу
- категориальные пропуски заполнены модой или специальным значением `"missing"`
- категориальные признаки закодированы через `OneHotEncoder`
- признаки приведены к одному масштабу с помощью `StandardScaler`
- данные разделены на train / test

Модель:

- реализован лосс `MSEL2Loss` = MSE + L2-регуляризация весов (без штрафа за bias)
- написан класс `LinearRegression`, который:
  - обучается **только** градиентным спуском поверх абстракции лосса
  - добавляет константный признак
  - хранит историю обучения (траекторию весов и значения лосса)

### Имитация отжига

- Реализован оптимизатор `simulated_annealing` для минимизации MSE:
  - новые точки предлагаются как шаг из распределения Стьюдента вокруг текущего вектора весов
  - температура охлаждается геометрически, с нижней границей по `t_min`
  - отдельно запоминается лучшее найденное решение
- Сравнение с градиентным спуском:
  - по траектории в пространстве весов
  - по скорости убывания MSE на train
  - по финальному качеству на test

## Наблюдения и выводы

- На 2D-игрушке градиентный спуск действительно снижает MSE с сотен до ~0.8: при слишком маленьком шаге обучение почти не движется, при слишком большом шаге траектория разлетается, и лосс уходит в бесконечность
- Стохастический градиентный спуск с мини-батчами ведёт себя шумнее, но при разумном степенном затухании шага выходит в тот же минимум, что и полный GD; без затухания или при слишком сильном затухании качество заметно хуже
- При этом стохастический для того же значения лоса тратит очень сильно меньше ресурсов
- На автомобильном датасете линейная регрессия, обучаемая градиентным спуском, даёт не совсем хорошие значения MSE; добавление L2-регуляризации немного снижает ошибку на test и не меняет (только в этом случае) train -> уменьшает переобучение
- Дополнительно проверялся Huber-лосс: на данных без ярко выраженных выбросов его качество близко к обычному MSE, но сама форма лосса лучше защищает модель от одиночных аномальных точек
- Имитация отжига на задаче линейной регрессии не даёт выигрыша по качеству по сравнению с градиентным спуском вообще примерно в 100 раз хуже в данном случае

